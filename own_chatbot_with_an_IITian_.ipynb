{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8079effdf2a4fdd8ee8e8eb74a60427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e81472665ab4d42a6a51b2ff9d7aaee",
              "IPY_MODEL_801d27225f5348e0aa7ea212fcdaa3e1",
              "IPY_MODEL_7a35e75289114db69d8078a18e55b21e"
            ],
            "layout": "IPY_MODEL_6a216f8b13e64c82bbd9fe7e886eb499"
          }
        },
        "9e81472665ab4d42a6a51b2ff9d7aaee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ea904d8f25b4bc1a18826ad700fa864",
            "placeholder": "​",
            "style": "IPY_MODEL_dd85d7fa233e4f588a9fe7d981b32714",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "801d27225f5348e0aa7ea212fcdaa3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f78732153934a51b712ddec8e446813",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d3b898e9e614efb835a403c0f3e8bdc",
            "value": 2
          }
        },
        "7a35e75289114db69d8078a18e55b21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8143fc5ce439466594df47f373a11f8e",
            "placeholder": "​",
            "style": "IPY_MODEL_4870aeb971cc4a4297f44756737f0915",
            "value": " 2/2 [00:00&lt;00:00, 91.47it/s]"
          }
        },
        "6a216f8b13e64c82bbd9fe7e886eb499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ea904d8f25b4bc1a18826ad700fa864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd85d7fa233e4f588a9fe7d981b32714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f78732153934a51b712ddec8e446813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d3b898e9e614efb835a403c0f3e8bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8143fc5ce439466594df47f373a11f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4870aeb971cc4a4297f44756737f0915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyH9Tvn5tHMA",
        "outputId": "31c582af-836b-4f27-c05d-e3af653ba0df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496,
          "referenced_widgets": [
            "d8079effdf2a4fdd8ee8e8eb74a60427",
            "9e81472665ab4d42a6a51b2ff9d7aaee",
            "801d27225f5348e0aa7ea212fcdaa3e1",
            "7a35e75289114db69d8078a18e55b21e",
            "6a216f8b13e64c82bbd9fe7e886eb499",
            "2ea904d8f25b4bc1a18826ad700fa864",
            "dd85d7fa233e4f588a9fe7d981b32714",
            "3f78732153934a51b712ddec8e446813",
            "9d3b898e9e614efb835a403c0f3e8bdc",
            "8143fc5ce439466594df47f373a11f8e",
            "4870aeb971cc4a4297f44756737f0915"
          ]
        },
        "id": "WzXIVER-sQJi",
        "outputId": "265878ff-f78f-499b-a10f-076b74319c64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8079effdf2a4fdd8ee8e8eb74a60427"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-3172936787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_for_auto_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_generation_mixin_to_remote_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    594\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4938\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4939\u001b[0;31m                 \u001b[0mdispatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdevice_map_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    505\u001b[0m                 \u001b[0;34m\"You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead."
          ]
        }
      ],
      "source": [
        "# Requirements:\n",
        "# pip install transformers accelerate peft sentence-transformers faiss-cpu\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline\n",
        ")\n",
        "from peft import PeftModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1. Model & LoRA adapter loading\n",
        "# ----------------------------------------\n",
        "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
        "LORA_WEIGHTS_PATH = \"./lora-falcon-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME, trust_remote_code=True\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "if os.path.isdir(LORA_WEIGHTS_PATH):\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        LORA_WEIGHTS_PATH,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Warning: LoRA directory '{LORA_WEIGHTS_PATH}' not found. Using base model.\")\n",
        "    model = base_model\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2. Text generation pipeline\n",
        "# ----------------------------------------\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3. Document Retrieval (RAG) setup\n",
        "# ----------------------------------------\n",
        "docs = [\n",
        "    \"Cristiano Ronaldo is one of the greatest footballers of all time.\",\n",
        "    \"Real Madrid was founded in the early 20th century.\",\n",
        "    \"The UEFA Champions League is Europe's premier club competition.\"\n",
        "]\n",
        "\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "doc_embeddings = embed_model.encode(docs, convert_to_numpy=True)\n",
        "\n",
        "faiss_index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
        "faiss_index.add(doc_embeddings)\n",
        "\n",
        "def retrieve_docs(query: str, top_k: int = 2):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = faiss_index.search(q_emb, top_k)\n",
        "    return [docs[i] for i in indices[0]]\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4. Dynamic memory summarization\n",
        "# ----------------------------------------\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "MAX_MEMORY_TOKENS = 512\n",
        "\n",
        "def dynamic_summarize(history: list) -> list:\n",
        "    text = \"\\n\".join(history)\n",
        "    summary = summarizer(\n",
        "        text, max_length=150, min_length=50, do_sample=False\n",
        "    )[0]['summary_text']\n",
        "    return [summary]\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5. Long-term Memory Bank\n",
        "# ----------------------------------------\n",
        "memory_bank = []\n",
        "memory_embeddings = []\n",
        "memory_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def extract_memory_detail(conversation: str, new_message: str) -> str | None:\n",
        "    prompt = (\n",
        "        f\"Conversation so far:\\n{conversation}\\n\\n\"\n",
        "        f\"New user message:\\n\\\"{new_message}\\\"\\n\\n\"\n",
        "        \"Extract any new personal detail or preference. \"\n",
        "        \"If none, reply exactly 'None'.\"\n",
        "    )\n",
        "    out = gen_pipeline(\n",
        "        prompt,\n",
        "        max_length=64,\n",
        "        do_sample=False,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    detail = out[0][\"generated_text\"].strip()\n",
        "    return detail if detail.lower() != \"none\" else None\n",
        "\n",
        "def add_to_memory_bank(detail: str):\n",
        "    emb = memory_encoder.encode([detail], convert_to_numpy=True)[0]\n",
        "    memory_bank.append(detail)\n",
        "    memory_embeddings.append(emb)\n",
        "\n",
        "def retrieve_memory(query: str, top_k: int = 2) -> list[str]:\n",
        "    if not memory_bank:\n",
        "        return []\n",
        "    q_emb = memory_encoder.encode([query], convert_to_numpy=True)[0]\n",
        "    sims = np.dot(memory_embeddings, q_emb) / (\n",
        "        np.linalg.norm(memory_embeddings, axis=1) * np.linalg.norm(q_emb) + 1e-8\n",
        "    )\n",
        "    top_idxs = sims.argsort()[-top_k:][::-1]\n",
        "    return [memory_bank[i] for i in top_idxs]\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6. Few-Shot Prompt Engineering\n",
        "# ----------------------------------------\n",
        "few_shot_examples = [\n",
        "    {\n",
        "        \"user\": \"Who is Ronaldo?\",\n",
        "        \"bot\": \"Cristiano Ronaldo is a Portuguese footballer widely regarded as one of the best.\"\n",
        "    },\n",
        "    {\n",
        "        \"user\": \"How many Champions League titles does Real Madrid have?\",\n",
        "        \"bot\": \"Real Madrid has won the UEFA Champions League 14 times.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def build_few_shot_block(n: int = 2) -> str:\n",
        "    picks = random.sample(few_shot_examples, n)\n",
        "    return \"\\n\".join(f\"Alice: {ex['user']}\\nBob: {ex['bot']}\" for ex in picks)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7. Hallucination Monitoring & Content Filtering\n",
        "# ----------------------------------------\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "HALLUCINATION_LABEL = \"contradiction\"\n",
        "BANNED_TERMS = [\"insult\", \"forbidden\"]\n",
        "\n",
        "def is_hallucination(context: str, response: str) -> bool:\n",
        "    seq = classifier(\n",
        "        response,\n",
        "        candidate_labels=[\"entailment\", \"neutral\", \"contradiction\"],\n",
        "        hypothesis_template=context\n",
        "    )\n",
        "    score = seq[\"scores\"][seq[\"labels\"].index(HALLUCINATION_LABEL)]\n",
        "    return score > 0.5\n",
        "\n",
        "def content_filter(response: str) -> bool:\n",
        "    return any(term in response for term in BANNED_TERMS)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8. Main Chat Loop\n",
        "# ----------------------------------------\n",
        "conversation_history = []\n",
        "\n",
        "print(\"Chatbot ready. Press Ctrl+C to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Alice: \").strip()\n",
        "    conversation_history.append(f\"Alice: {user_input}\")\n",
        "\n",
        "    # long-term memory extraction\n",
        "    full_history = \"\\n\".join(conversation_history)\n",
        "    memory_detail = extract_memory_detail(full_history, user_input)\n",
        "    if memory_detail:\n",
        "        add_to_memory_bank(memory_detail)\n",
        "\n",
        "    # dynamic summarization if too long\n",
        "    token_count = len(tokenizer(\"\\n\".join(conversation_history))[\"input_ids\"])\n",
        "    if token_count > MAX_MEMORY_TOKENS:\n",
        "        conversation_history = dynamic_summarize(conversation_history)\n",
        "\n",
        "    # retrieve memory and docs\n",
        "    mem_items = retrieve_memory(user_input, top_k=2)\n",
        "    mem_block = \"\\n\".join(f\"[Memory {i+1}]: {m}\" for i, m in enumerate(mem_items))\n",
        "\n",
        "    doc_items = retrieve_docs(user_input, top_k=2)\n",
        "    doc_block = \"\\n\".join(f\"[Context {i+1}]: {d}\" for i, d in enumerate(doc_items))\n",
        "\n",
        "    # build few-shot examples\n",
        "    few_shot_block = build_few_shot_block(n=2)\n",
        "\n",
        "    # assemble prompt\n",
        "    history_block = \"\\n\".join(conversation_history)\n",
        "    prompt_parts = [mem_block, doc_block, few_shot_block, history_block]\n",
        "    prompt = \"\\n\".join(part for part in prompt_parts if part) + \"\\nBob:\"\n",
        "\n",
        "    # generate response\n",
        "    out = gen_pipeline(\n",
        "        prompt,\n",
        "        max_length=512,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        return_full_text=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    bot_response = out[0][\"generated_text\"].strip()\n",
        "\n",
        "    # hallucination & filter check\n",
        "    if is_hallucination(history_block, bot_response) or content_filter(bot_response):\n",
        "        print(\"Bob: Sorry, I couldn't generate a reliable answer. Please rephrase your question.\")\n",
        "    else:\n",
        "        print(\"Bob:\", bot_response)\n",
        "        conversation_history.append(f\"Bob: {bot_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAJd45WDs9RY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}